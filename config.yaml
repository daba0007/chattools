llm_type: "glm6b"
#  LLM模型类型:glm6b、rwkv
glm:
  path: ""
  # glm模型位置
  strategy: ""
  # glm 模型参数  支持：
  # "cpu fp32"  所有glm模型 要直接跑在cpu上都可以使用这个参数
  # "cpu fp32i8" fp16原生模型 要自行量化为int8跑在cpu上可以使用这个参数
  # "cpu fp32i4" fp16原生模型要 自行量化为int4跑在cpu上可以使用这个参数
  # "cuda fp16"  所有glm模型 要直接跑在gpu上都可以使用这个参数
  # "cuda fp16i8"  fp16原生模型 要自行量化为int8跑在gpu上可以使用这个参数
  # "cuda fp16i4"  fp16原生模型 要自行量化为int4跑在gpu上可以使用这个参数
llama:
  path: ""
  # llama模型位置
  strategy: ""
  # 暂无策略
lora: ""
fess:
  path: "localhost:8080"
  count: 5
bing:
  count: 5
weight:
# 消息采集比重
  bing: 1
  fess: 9